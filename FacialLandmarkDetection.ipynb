{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6f7129-eefd-404b-8599-03f8defcae13",
   "metadata": {},
   "source": [
    "**Date: 20-10-2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e580e27e-35ce-4299-b5d8-ebcf24a57bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048c6a9-063d-4882-b199-9fe5045d1840",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "985db878-d211-4253-9070-615f812501c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialLandmarkDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def read_pts(self, pts_path):\n",
    "        points = []\n",
    "        with open(pts_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Skip metadata or braces\n",
    "                if line.startswith(\"version\") or line.startswith(\"n_points\") or line.startswith(\"{\") or line.startswith(\"}\"):\n",
    "                    continue\n",
    "                try:\n",
    "                    x, y = map(float, line.split())\n",
    "                    points.append([x, y])\n",
    "                except ValueError:\n",
    "                    continue  # Skip lines that can't be parsed\n",
    "        return np.array(points).flatten()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        base_name = os.path.splitext(self.image_files[idx])[0]\n",
    "        #pts_path = os.path.join(self.annotation_dir, self.image_files[idx].replace('.jpg', '.pts'))\n",
    "        pts_path = os.path.join(self.annotation_dir,base_name+'.pts')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image = image / 255.0\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        landmarks = self.read_pts(pts_path)\n",
    "        landmarks = torch.tensor(landmarks, dtype=torch.float32)\n",
    "\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0105667-d912-447d-8108-7dad3a2e343d",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a594eb9-6220-42b1-bb17-8172714f0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkModel(nn.Module):\n",
    "    def __init__(self, num_landmarks):\n",
    "        super(LandmarkModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 224, 224)\n",
    "            dummy_output = self.cnn(dummy_input)\n",
    "            self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_landmarks * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39da8d-c668-4227-9ee3-f4e7ebdc4602",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8427ead-98ca-43d0-82e1-2b9d921c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, landmarks in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, landmarks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88982d-fb8d-4c03-87a2-e45536a015b8",
   "metadata": {},
   "source": [
    "## Testing with WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5da992bf-c58d-495e-8bf7-33e9e69d51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_webcam(model):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "            roi = cv2.resize(roi, (224, 224))\n",
    "            roi = roi.astype(np.float32) / 255.0\n",
    "            roi = (roi - 0.5) / 0.5  # Optional: if you normalized during training\n",
    "            roi = torch.tensor(roi).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(roi).detach().cpu().numpy().reshape(-1, 2)\n",
    "                output *= [w / 224, h / 224]  # scale to face size\n",
    "                output += [x, y]              # shift to original frame\n",
    "\n",
    "            for (lx, ly) in output:\n",
    "                cv2.circle(frame, (int(lx), int(ly)), 2, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.imshow('Facial Landmark Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07e30f-3670-4d5c-b6f6-eef5497012c1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4154479-29cb-4fca-8695-6525446bdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, landmarks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Evaluation Loss (MSE): {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc352388-16d8-4111-9669-2de04da60992",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f91014ea-f0c5-4248-aaf7-6ca33882bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of landmarks: 68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Initialize and train\u001b[39;00m\n\u001b[32m      8\u001b[39m model = LandmarkModel(num_landmarks)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     12\u001b[39m evaluate_model(model, dataloader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, epochs)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m      6\u001b[39m     running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Music\\fld\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Music\\fld\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Music\\fld\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mFacialLandmarkDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#pts_path = os.path.join(self.annotation_dir, self.image_files[idx].replace('.jpg', '.pts'))\u001b[39;00m\n\u001b[32m     30\u001b[39m pts_path = os.path.join(\u001b[38;5;28mself\u001b[39m.annotation_dir,base_name+\u001b[33m'\u001b[39m\u001b[33m.pts\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\u001b[32m     34\u001b[39m image = cv2.resize(image, (\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = FacialLandmarkDataset('dataset/images/', 'dataset/annotations')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "image, landmarks = dataset[0]\n",
    "num_landmarks = landmarks.shape[0] // 2\n",
    "print(f\"No of landmarks: {num_landmarks}\")\n",
    "# Initialize and train\n",
    "model = LandmarkModel(num_landmarks)\n",
    "train_model(model, dataloader, epochs=20)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, dataloader)\n",
    "\n",
    "# Test on webcam\n",
    "#test_webcam(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc88f94-ac99-4fb9-bd48-3d118155a40c",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "238f2fa8-6cb8-4e6d-ab14-70529ade4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to landmark_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model after training\n",
    "torch.save(model.state_dict(), 'landmark_model.pth')\n",
    "print(\"Model saved to landmark_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2ef6a-dac0-4afe-81af-ba3aa49d191c",
   "metadata": {},
   "source": [
    "## Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43c955b2-0ede-40e0-8578-1f26311ce6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for webcam testing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Recreate the model class\n",
    "class LandmarkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LandmarkModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 224, 224)\n",
    "            dummy_output = self.cnn(dummy_input)\n",
    "            self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 136)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load model\n",
    "model = LandmarkModel()\n",
    "model.load_state_dict(torch.load('landmark_model.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded and ready for webcam testing\")\n",
    "\n",
    "# Start webcam detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (224, 224))\n",
    "        roi = roi / 255.0\n",
    "        roi = torch.tensor(roi, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(roi).numpy().reshape(-1, 2)\n",
    "            output *= [w / 224, h / 224]\n",
    "            output += [x, y]\n",
    "\n",
    "        for (lx, ly) in output:\n",
    "            cv2.circle(frame, (int(lx), int(ly)), 2, (0, 255, 0), -1)\n",
    "\n",
    "    cv2.imshow('Facial Landmark Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54ecc4ac-8c95-4202-bcec-361cae4cbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenCV DNN face detector\n",
    "face_net = cv2.dnn.readNetFromCaffe(\n",
    "    'deploy.prototxt',  # Download from OpenCV GitHub\n",
    "    'res10_300x300_ssd_iter_140000.caffemodel'  # Download from OpenCV GitHub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fb19cd3-6123-4e20-8b2f-de2a09199d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LandmarkModel(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=86528, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=136, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LandmarkModel(num_landmarks)  # Use correct number from earlier\n",
    "model.load_state_dict(torch.load('landmark_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9b241-a9b4-40cc-8963-d6ea01735418",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Step 1: Detect face using OpenCV DNN\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.6:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            x1, y1, x2, y2 = box.astype(\"int\")\n",
    "\n",
    "            # âœ… Step 2: Crop and preprocess face\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (224, 224))\n",
    "            normalized = resized.astype(np.float32) / 255.0\n",
    "            tensor = torch.tensor(normalized).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Step 3: Predict landmarks\n",
    "            with torch.no_grad():\n",
    "                preds = model(tensor).cpu().numpy().reshape(-1, 2)\n",
    "                preds *= [(x2 - x1) / 224, (y2 - y1) / 224]\n",
    "                preds += [x1, y1]\n",
    "\n",
    "            # Step 4: Draw landmarks\n",
    "            for (lx, ly) in preds:\n",
    "                cv2.circle(frame, (int(lx), int(ly)), 2, (0, 255, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"Facial Landmark Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
